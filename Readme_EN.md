# eland Internship learning note

This project records the content and experience of the internship in the RD2 of eland. In this internship, I participated in two NLP-related projects, namely "NER Project - Government Unit Label Production" and "Automatic Summary". The code and usage are in the folder.

- [NLP_Introduction](#NLP_Introduction)
     - [Seq2Seq](#Seq2Seq)
- [NER_Project](#NER_Project)
- [Abstractive_summary](#Abstractive_summary)
     - [T5](#T5)
     - [mT5](#mT5)
     - [generation_algorithm](#generation_algorithm)
     - [GPT2](#GPT2)
- [References](#References)

## NLP_Introduction
Natural language processing (NLP) is a technology that enables machines to recognize, understand and use our language through complex mathematical models and algorithms.

The early NLP technology was mainly based on statistical concepts to train the model, allowing the machine to read a large amount of data and calculate the probability of the occurrence of words and sentences. However, this method cannot make the system recognize complex grammar well and the generated content is not smooth.

The emergence of deep learning has changed the operation mode of training NLP in the past, and the algorithm model most widely used by researchers is BERT. The full name of BERT is Bidirectional Encoder Representations from Transformers.

BERT uses a pre-trained algorithm to check the front and rear words in both directions, and then infer the complete context. This approach is different from the previous model, which can connect the context more comprehensively and effectively help the system understand and generate text.

### Seq2Seq
Seq2Seq consists of an Encoder and a Decoder. Encoder takes an input sequence and maps it to a higher dimensional space (n-dimensional vector). This abstract vector is fed into the Decoder, which converts it into an output sequence.
We can think of Encoder and Decoder as human translators who can only speak two languages. Their first language is their native language (eg Chinese and English) and their second language is their shared imaginary language. In order to translate Chinese into English, the Encoder converts the Chinese sentence into an imaginary language, and then the Decoder reads the imaginary language and translates it into English.

Since the Encoder and the Decoder are both imaginary languages, and they are not very fluent at the beginning, we need to let them do enough learning before translating, that is, pre-training.

In addition, instead of just handing the last "one" vector generated by the Encoder to the Decoder and asking it to extract the entire sentence information, it is better to pass the "all" output vectors generated by the Encoder after processing each word to the Decoder. Let the Decoder decide which output vectors of the Encoder to put "attention" on when generating a new sequence.

## NER_Project

The first project of the internship is related to the work of proper noun recognition, or Named Entity Recognition (NER), which is to use machine learning to identify words in text, such as person names, place names, organizations, etc.

![image](./image/ner1.png)

In this project, we will separate government units from the organization and create labels for government units.


## Abstractive_summary

In the automatic summarization project we used two models (mT5 and GPT2).

mT5 is a multi-language version of T5, so we will introduce T5 first.

### T5
T5 is the abbreviation of Text-To-Text Transfer Transformer. The data is pre-trained first, and the pre-trained model parameters are used to fine-tune the real target field. T5 is suitable for many NLP-related tasks, such as translation, classification, regression (for example, predicting how similar two sentences are, with a similarity score between 1 and 5), and other seq2seq tasks, such as summarizing, generating articles.

T5 uses C4 (Colossal Clean Crawled Corpus) data in the pre-training process. C4 removes duplicate data and incomplete sentences after crawling web articles to make the database clean enough. During pre-training, the C4 data set is corrupted spans

Unlike GPT2, T5 includes Encoder and Decoder, while GPT2 only has Decoder.

What T5 can do
- Translate
- Q&A
- Classification
- Summary
- Regression

How to do Summary task?

The decoder is trained to predict the next word in the sequence given the previous word.

Here are the steps to decode the test sequence:

1. Encode the entire input sequence and initialize the decoder with the encoder's internal state
2. Pass the < start > tag as input to the decoder
3. Run the decoder for one time step using the internal state
4. The output will be the probability of the next word. The word with the highest probability will be selected
5. Pass the sampled word as input to the decoder in the next time step and update the internal state with the current time step
6. Repeat steps 3 - 5 until we generate an < end > tag or reach the maximum length of the target sequence

### mT5

The difference between mT5 and T5 is that it needs to be fine-tuned to be used. No prefix is required in single-task fine-tuning, but a prefix is required in multi-task fine-tuning.

mT5 is pre-trained on the mC4 corpus covering 101 languages.

### generation_algorithm

The generated summary will have different results due to different algorithms. The following introduces 4 algorithms that can be used by the pipeline.

- Greedy search
    - `do_sample = False`
    - `num_beams = 1`
    - Choose the highest probability
    - The simplest algorithm, but the content produced is limited by the training data.
- Random sampling
    - `do_sample = True`
    - `num_beams = 1`
    - According to the probability of the word, and randomly pick one.
    - Can be used with `temperature`, `temperature` will increase the probability of words with high probability and reduce the probability of words with low probability.
     - `temperature = 1` -> Random sampling
     - The probability of `0 < temperature < 1` -> word will be adjusted, the smaller the value, the greater the effect
     - `temperature -> 0` -> Greedy search
     - Can be used with `top_k`, `top_p`
     - `top_p` can solve the problem of using `top_k` to have a certain number of candidate words each time
- Beams search
    - `do_sample = False`
    - `num_beams > 1`
    - The top n results will be kept until the end, which can be used to generate multiple results.
     - Applicable when the length of each output is similar
     - Severely affected by duplicate spawns
     - Insufficient randomness, different from human everyday

- Beams sampling
    - `do_sample = True`
    - `num_beams > 1`
    - Use a combination of Beams and sampling methods.
     - During the use of mt5, if both `top_k` and  `temperature` are used, model will sometimes generate the results in other languages

### GPT2

The predecessor of GPT-2 is GPT, which full name is Generative Pre-Training. In the GPT-2 paper, the authors first crawled nearly 40 GB of text data called WebText (open source version) from the Internet, and used this huge text to train several language models based on the Transformer architecture. ), allowing these models to predict the next word after reading in a piece of text.

It is worth mentioning that GPT proposed by OpenAI and BERT, Google's language representation model, both believe in two-stage transfer learning: using a large amount of text to train a general NLP model with high natural language understanding ability. With such a general model, a wide variety of NLP tasks can then be solved by simply fine-tuning the same model, without the need to design specific neural network architectures for different tasks each time, saving time, effort and efficiency .

The difference between the two lies in the training targets and models used for unsupervised training:

- GPT selects the Decoder in Transformer, the training target is a general language model, and the next word is predicted
- BERT selects the Encoder in Transformer, and the training target is to fill in the blanks and predict the next sentence

GPT-2 is also used in various NLP tasks such as reading comprehension, translation, summarization, and question answering tasks. For example, This Waifu Does Not Exist uses GAN to generate anime avatars and also uses GPT-2 to randomly generate an anime plot; and TabNine is a Canadian team using GPT-2 as a smart auto-complete development tool, aiming to let engineers They reduce unnecessary typing and even recommend better writing.

## References
1. [T5 video -> Colin Raffel](https://www.youtube.com/watch?v=eKqWC577WlI&list=UUEqgmyWChwvt6MFGGlmUQCQ&index=5)
2. [Decoding Strategies that You Need to Know for Response Generation](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc)
3. [淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html?fbclid=IwAR2eHxhPxyg96A3mbtveRHd5zFKscSLA-u8jdoDueUC9Dl1g3Vrv-61Y84g)
4. [直觀理解GPT2](https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html)
